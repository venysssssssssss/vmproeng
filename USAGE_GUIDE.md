# Projeto de Engenharia de Dados - Guia de Uso

## ğŸš€ Quick Start

### 1. Iniciar o Projeto

```bash
# Dar permissÃ£o de execuÃ§Ã£o aos scripts
chmod +x start.sh stop.sh

# Iniciar todos os serviÃ§os
./start.sh
```

Aguarde aproximadamente 30-60 segundos para todos os serviÃ§os iniciarem.

### 2. Acessar as Interfaces

- **Airflow**: http://localhost:8080
  - UsuÃ¡rio: `admin`
  - Senha: `admin`

- **Spark Master**: http://localhost:8081
  - Monitorar jobs Spark

- **MinIO Console**: http://localhost:9001
  - UsuÃ¡rio: `minioadmin`
  - Senha: `minioadmin`

- **Metabase**: http://localhost:3000
  - Configurar no primeiro acesso

### 3. Executar Pipeline de Dados

#### Pelo Airflow UI:
1. Acesse http://localhost:8080
2. FaÃ§a login (admin/admin)
3. Ative a DAG `ecommerce_etl_pipeline`
4. Clique no botÃ£o "Trigger DAG" (Ã­cone de play)

#### Por linha de comando:
```bash
docker-compose exec airflow-webserver airflow dags trigger ecommerce_etl_pipeline
```

## ğŸ“Š Pipelines DisponÃ­veis

### 1. ecommerce_etl_pipeline
**FrequÃªncia**: DiÃ¡ria  
**DescriÃ§Ã£o**: Pipeline completo de ETL para dados de e-commerce

**Etapas**:
1. Gera dados de transaÃ§Ãµes de vendas
2. Carrega para camada raw
3. Limpa e transforma dados
4. Carrega dimensÃµes
5. Carrega tabela fato
6. Cria agregaÃ§Ãµes analÃ­ticas

### 2. api_ingestion
**FrequÃªncia**: A cada 6 horas  
**DescriÃ§Ã£o**: Ingere dados de clientes de API externa

**Etapas**:
1. Busca dados de clientes
2. Salva em formato JSON
3. Carrega para banco de dados
4. Atualiza tabela de dimensÃµes

### 3. data_quality_check
**FrequÃªncia**: DiÃ¡ria  
**DescriÃ§Ã£o**: Verifica qualidade dos dados usando Spark

**VerificaÃ§Ãµes**:
- Valores nulos
- Duplicatas
- Valores negativos
- ConsistÃªncia de dados
- DistribuiÃ§Ã£o de categorias

### 4. minio_datalake
**FrequÃªncia**: DiÃ¡ria  
**DescriÃ§Ã£o**: Gerencia data lake no MinIO

**Etapas**:
1. Cria buckets necessÃ¡rios
2. Upload de dados para S3
3. Lista objetos armazenados

## ğŸ—„ï¸ Estrutura do Data Warehouse

### Camadas de Dados

```
raw/               # Dados brutos da origem
â”œâ”€â”€ sales_transactions
â””â”€â”€ customer_data

staging/           # Dados limpos e validados
â””â”€â”€ sales_clean

processed/         # Modelo dimensional (Star Schema)
â”œâ”€â”€ dim_customers
â”œâ”€â”€ dim_products
â”œâ”€â”€ dim_date
â””â”€â”€ fact_sales

analytics/         # AgregaÃ§Ãµes e mÃ©tricas
â”œâ”€â”€ daily_sales_summary
â”œâ”€â”€ product_performance
â””â”€â”€ customer_metrics
```

## ğŸ“ˆ Conectar Metabase ao PostgreSQL

1. Acesse http://localhost:3000
2. Configure usuÃ¡rio e senha
3. Adicione banco de dados:
   - **Tipo**: PostgreSQL
   - **Host**: postgres
   - **Porta**: 5432
   - **Database**: datawarehouse
   - **UsuÃ¡rio**: postgres
   - **Senha**: postgres

4. Use as queries em `sql/analytics_queries.sql` para criar dashboards

## ğŸ” Monitoramento

### Ver logs dos serviÃ§os

```bash
# Todos os serviÃ§os
docker-compose logs -f

# ServiÃ§o especÃ­fico
docker-compose logs -f airflow-webserver
docker-compose logs -f postgres
docker-compose logs -f spark-master
```

### Status dos containers

```bash
docker-compose ps
```

### Uso de recursos

```bash
docker stats
```

## ğŸ› ï¸ Comandos Ãšteis

### Airflow

```bash
# Listar DAGs
docker-compose exec airflow-webserver airflow dags list

# Executar DAG especÃ­fica
docker-compose exec airflow-webserver airflow dags trigger <dag_id>

# Ver tarefas de uma DAG
docker-compose exec airflow-webserver airflow tasks list <dag_id>

# Criar usuÃ¡rio admin
docker-compose exec airflow-webserver airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com
```

### PostgreSQL

```bash
# Conectar ao banco
docker-compose exec postgres psql -U postgres -d datawarehouse

# Executar query
docker-compose exec postgres psql -U postgres -d datawarehouse -c "SELECT COUNT(*) FROM processed.fact_sales;"

# Backup do banco
docker-compose exec postgres pg_dump -U postgres datawarehouse > backup.sql

# Restore do banco
docker-compose exec -T postgres psql -U postgres datawarehouse < backup.sql
```

### Spark

```bash
# Submeter job Spark
docker-compose exec spark-master spark-submit \
    --master spark://spark-master:7077 \
    --driver-memory 256m \
    --executor-memory 256m \
    /opt/spark-jobs/data_quality_check.py
```

### MinIO

```bash
# Listar buckets
docker-compose exec minio mc ls minio

# Criar bucket
docker-compose exec minio mc mb minio/my-bucket
```

## ğŸ”§ Troubleshooting

### ServiÃ§os nÃ£o iniciam

```bash
# Verificar logs
docker-compose logs

# Reiniciar serviÃ§os
docker-compose restart

# Parar e remover tudo
docker-compose down -v
./start.sh
```

### Pouca memÃ³ria disponÃ­vel

```bash
# Parar serviÃ§os opcionais
docker-compose stop metabase
docker-compose stop spark-worker

# Verificar uso de memÃ³ria
free -h
docker stats
```

### Airflow nÃ£o conecta ao banco

```bash
# Reinicializar banco do Airflow
docker-compose exec airflow-webserver airflow db reset
docker-compose exec airflow-webserver airflow db init
```

### Limpar dados antigos

```bash
# Limpar logs do Airflow
rm -rf airflow/logs/*

# Limpar dados raw
rm -rf data/raw/*
rm -rf data/staging/*
rm -rf data/processed/*

# Resetar banco (CUIDADO: apaga todos os dados)
docker-compose down -v
./start.sh
```

## ğŸ“¦ Adicionar Novas DAGs

1. Criar arquivo Python em `airflow/dags/`
2. Seguir estrutura padrÃ£o do Airflow
3. DAG serÃ¡ detectada automaticamente (pode levar ~1 minuto)
4. Verificar no Airflow UI

## ğŸ›‘ Parar o Projeto

```bash
# Parar serviÃ§os (mantÃ©m dados)
./stop.sh

# Parar e limpar dados
./stop.sh --clean

# Ou manualmente
docker-compose down        # Para serviÃ§os
docker-compose down -v     # Para serviÃ§os e remove volumes
```

## ğŸ“Š Exemplos de AnÃ¡lises

### Query 1: Vendas dos Ãºltimos 7 dias
```sql
SELECT 
    d.full_date,
    COUNT(*) as transactions,
    SUM(f.total_amount) as revenue
FROM processed.fact_sales f
JOIN processed.dim_date d ON f.date_key = d.date_key
WHERE d.full_date >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY d.full_date
ORDER BY d.full_date;
```

### Query 2: Top 5 produtos
```sql
SELECT 
    p.product_name,
    p.category,
    SUM(f.total_amount) as revenue
FROM processed.fact_sales f
JOIN processed.dim_products p ON f.product_key = p.product_key
GROUP BY p.product_name, p.category
ORDER BY revenue DESC
LIMIT 5;
```

## ğŸ¯ PrÃ³ximos Passos

1. âœ… Configurar alertas no Airflow
2. âœ… Implementar testes de dados (Great Expectations)
3. âœ… Adicionar mais fontes de dados
4. âœ… Criar dashboards no Metabase
5. âœ… Implementar CDC (Change Data Capture)
6. âœ… Adicionar data lineage tracking

## ğŸ“š Recursos Adicionais

- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Spark Documentation](https://spark.apache.org/docs/latest/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [MinIO Documentation](https://min.io/docs/)
- [Metabase Documentation](https://www.metabase.com/docs/)
