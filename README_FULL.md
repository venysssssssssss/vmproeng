# ğŸš€ Data Engineering Project - Complete Pipeline

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![Airflow](https://img.shields.io/badge/Airflow-2.8.0-orange.svg)](https://airflow.apache.org/)
[![Spark](https://img.shields.io/badge/Spark-3.5-red.svg)](https://spark.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)](https://www.postgresql.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> Projeto completo de Engenharia de Dados com pipeline moderno de ETL/ELT, otimizado para ambientes com recursos limitados (2 CPUs, 1GB RAM).

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Arquitetura](#-arquitetura)
- [Tecnologias](#-tecnologias)
- [Quick Start](#-quick-start)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Pipelines DisponÃ­veis](#-pipelines-disponÃ­veis)
- [Uso](#-uso)
- [DocumentaÃ§Ã£o](#-documentaÃ§Ã£o)
- [Contribuindo](#-contribuindo)

## ğŸ¯ VisÃ£o Geral

Este projeto implementa uma pipeline completa de engenharia de dados seguindo as melhores prÃ¡ticas da indÃºstria:

- âœ… **Arquitetura em Camadas**: Raw â†’ Staging â†’ Processed â†’ Analytics
- âœ… **Modelo Dimensional**: Star Schema para anÃ¡lise eficiente
- âœ… **OrquestraÃ§Ã£o**: Apache Airflow para workflows automatizados
- âœ… **Processamento**: Apache Spark para transformaÃ§Ãµes em larga escala
- âœ… **Armazenamento**: PostgreSQL + MinIO (S3-compatible)
- âœ… **VisualizaÃ§Ã£o**: Metabase para BI e dashboards
- âœ… **Qualidade de Dados**: ValidaÃ§Ãµes automatizadas
- âœ… **ContainerizaÃ§Ã£o**: Docker para portabilidade

## ğŸ—ï¸ Arquitetura

```
Sources â†’ Ingestion â†’ Raw â†’ Staging â†’ Processed â†’ Analytics â†’ Visualization
          (Airflow)        (Clean)   (Star Schema)  (KPIs)    (Metabase)
```

**Componentes principais:**
- **Apache Airflow**: OrquestraÃ§Ã£o de workflows
- **Apache Spark**: Processamento distribuÃ­do
- **PostgreSQL**: Data warehouse
- **MinIO**: Object storage (data lake)
- **Redis**: Message broker
- **Metabase**: Business Intelligence

Veja [ARCHITECTURE.md](ARCHITECTURE.md) para detalhes completos.

## ğŸ› ï¸ Tecnologias

| Categoria | Tecnologia | VersÃ£o |
|-----------|------------|--------|
| **OrquestraÃ§Ã£o** | Apache Airflow | 2.8.0 |
| **Processamento** | Apache Spark | 3.5 |
| **Data Warehouse** | PostgreSQL | 15 |
| **Object Storage** | MinIO | Latest |
| **Message Broker** | Redis | 7 |
| **BI Tool** | Metabase | Latest |
| **Linguagem** | Python | 3.11 |
| **Container** | Docker | Latest |

## ğŸš€ Quick Start

### PrÃ©-requisitos

- Docker Engine 20.10+
- Docker Compose 2.0+
- 1GB RAM disponÃ­vel
- 2 CPU cores

### InstalaÃ§Ã£o

```bash
# 1. Clone o repositÃ³rio
git clone <repository-url>
cd vmpro1

# 2. Inicie os serviÃ§os
chmod +x start.sh
./start.sh

# 3. Aguarde ~60 segundos para os serviÃ§os iniciarem
```

### Acesso Ã s Interfaces

| ServiÃ§o | URL | Credenciais |
|---------|-----|-------------|
| **Airflow** | http://localhost:8080 | admin / admin |
| **Spark Master** | http://localhost:8081 | - |
| **MinIO Console** | http://localhost:9001 | minioadmin / minioadmin |
| **Metabase** | http://localhost:3000 | Configure no primeiro acesso |

### Executar Pipeline

```bash
# Via Makefile
make trigger-etl

# Ou diretamente
docker-compose exec airflow-webserver airflow dags trigger ecommerce_etl_pipeline
```

## ğŸ“ Estrutura do Projeto

```
vmpro1/
â”œâ”€â”€ airflow/                    # Airflow DAGs e configuraÃ§Ãµes
â”‚   â”œâ”€â”€ dags/                   # Pipeline definitions
â”‚   â”‚   â”œâ”€â”€ ecommerce_etl_pipeline.py
â”‚   â”‚   â”œâ”€â”€ api_ingestion_dag.py
â”‚   â”‚   â”œâ”€â”€ data_quality_dag.py
â”‚   â”‚   â””â”€â”€ minio_datalake_dag.py
â”‚   â”œâ”€â”€ logs/                   # Execution logs
â”‚   â””â”€â”€ plugins/                # Custom plugins
â”‚
â”œâ”€â”€ spark/                      # Spark jobs
â”‚   â”œâ”€â”€ jobs/                   # PySpark scripts
â”‚   â”‚   â”œâ”€â”€ data_quality_check.py
â”‚   â”‚   â””â”€â”€ sales_aggregation.py
â”‚   â””â”€â”€ notebooks/              # Jupyter notebooks
â”‚
â”œâ”€â”€ data/                       # Data storage
â”‚   â”œâ”€â”€ raw/                    # Raw data
â”‚   â”œâ”€â”€ staging/                # Cleaned data
â”‚   â””â”€â”€ processed/              # Processed data
â”‚
â”œâ”€â”€ sql/                        # SQL scripts
â”‚   â”œâ”€â”€ init/                   # Database initialization
â”‚   â”‚   â””â”€â”€ 01_init_db.sql
â”‚   â””â”€â”€ analytics_queries.sql   # Analytics queries
â”‚
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ airflow.cfg
â”‚
â”œâ”€â”€ docker-compose.yml          # Container orchestration
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Makefile                    # Automation commands
â”œâ”€â”€ start.sh                    # Startup script
â”œâ”€â”€ stop.sh                     # Shutdown script
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ README.md                   # Este arquivo
â”œâ”€â”€ ARCHITECTURE.md             # DocumentaÃ§Ã£o de arquitetura
â”œâ”€â”€ USAGE_GUIDE.md              # Guia de uso detalhado
â””â”€â”€ ROADMAP.md                  # Roadmap de melhorias
```

## ğŸ“Š Pipelines DisponÃ­veis

### 1. E-commerce ETL Pipeline
- **Schedule**: DiÃ¡rio
- **DescriÃ§Ã£o**: Pipeline completo de vendas (extraÃ§Ã£o â†’ transformaÃ§Ã£o â†’ carregamento)
- **Etapas**: Gera dados â†’ Raw â†’ Staging â†’ DimensÃµes â†’ Fato â†’ Analytics

### 2. API Data Ingestion
- **Schedule**: A cada 6 horas
- **DescriÃ§Ã£o**: Ingere dados de clientes de APIs externas
- **Etapas**: Fetch API â†’ JSON â†’ Database â†’ DimensÃµes

### 3. Data Quality Check
- **Schedule**: DiÃ¡rio
- **DescriÃ§Ã£o**: Verifica qualidade usando Spark
- **ValidaÃ§Ãµes**: Nulls, duplicatas, valores negativos, consistÃªncia

### 4. MinIO Data Lake
- **Schedule**: DiÃ¡rio
- **DescriÃ§Ã£o**: Gerencia data lake no MinIO
- **Etapas**: Cria buckets â†’ Upload â†’ CatalogaÃ§Ã£o

## ğŸ’» Uso

### Comandos Makefile

```bash
make help              # Lista todos os comandos
make start             # Inicia serviÃ§os
make stop              # Para serviÃ§os
make logs              # Visualiza logs
make status            # Status dos containers
make trigger-etl       # Executa pipeline ETL
make backup-db         # Backup do banco
make shell-postgres    # Acessa PostgreSQL
make clean-data        # Limpa dados temporÃ¡rios
```

### Comandos Manuais

```bash
# Ver logs do Airflow
docker-compose logs -f airflow-webserver

# Executar DAG
docker-compose exec airflow-webserver airflow dags trigger <dag_id>

# Acessar PostgreSQL
docker-compose exec postgres psql -U postgres -d datawarehouse

# Parar tudo e limpar
./stop.sh --clean
```

## ğŸ“š DocumentaÃ§Ã£o

- **[ARCHITECTURE.md](ARCHITECTURE.md)**: Arquitetura detalhada do sistema
- **[USAGE_GUIDE.md](USAGE_GUIDE.md)**: Guia completo de uso
- **[ROADMAP.md](ROADMAP.md)**: Melhorias futuras

## ğŸ“ Casos de Uso

1. **E-commerce Analytics**: AnÃ¡lise de vendas, produtos e clientes
2. **Data Quality Monitoring**: ValidaÃ§Ã£o automatizada de dados
3. **Data Lake Management**: Armazenamento em object storage
4. **BI Dashboards**: VisualizaÃ§Ãµes no Metabase
5. **ETL Automation**: Pipelines orquestrados pelo Airflow

## ğŸ”§ ConfiguraÃ§Ã£o do Ambiente

### Recursos Otimizados para Low Memory

```yaml
PostgreSQL:
  shared_buffers: 64MB
  max_connections: 50
  
Airflow:
  executor: LocalExecutor
  parallelism: 4
  workers: 2

Spark:
  driver_memory: 256m
  executor_memory: 256m
  executor_cores: 1
```

## ğŸ› Troubleshooting

### Problemas Comuns

**ServiÃ§os nÃ£o iniciam:**
```bash
docker-compose logs
docker-compose restart
```

**Erro de memÃ³ria:**
```bash
# Parar serviÃ§os opcionais
docker-compose stop metabase
```

**Airflow nÃ£o conecta:**
```bash
docker-compose exec airflow-webserver airflow db reset
```

Veja [USAGE_GUIDE.md](USAGE_GUIDE.md) para mais detalhes.

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ‘¥ Autores

- **Data Engineering Team** - *Initial work*

## ğŸ™ Agradecimentos

- Apache Software Foundation (Airflow, Spark)
- PostgreSQL Global Development Group
- MinIO Project
- Metabase Team
- Docker Community

## ğŸ“ Suporte

Para questÃµes e suporte:
- Abra uma issue no GitHub
- Consulte a documentaÃ§Ã£o em `/docs`
- Entre em contato: admin@example.com

---

**â­ Se este projeto foi Ãºtil, considere dar uma estrela!**
