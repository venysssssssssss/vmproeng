# ğŸ“¦ Data Engineering Pipeline - Projeto Otimizado

## âœ¨ O Que Foi Feito

### 1. ConsolidaÃ§Ã£o de DocumentaÃ§Ã£o
- âœ… **Removidos 10 arquivos** de documentaÃ§Ã£o redundante
- âœ… **Criado README.md** completo e conciso
- âœ… **Criado QUICKSTART.md** para inÃ­cio rÃ¡pido
- âœ… **Criado .env.example** com variÃ¡veis de ambiente

### 2. ConsolidaÃ§Ã£o de Scripts
- âœ… **Removidos 5 scripts** dispersos (start.sh, stop.sh, etc)
- âœ… **Criado utils.sh** Ãºnico com 12 comandos Ãºteis
- âœ… Comandos organizados por categoria (gerenciamento, monitoramento, dados)

### 3. Limpeza de Arquivos
- âœ… Removidos Makefile, poetry.lock, pyproject.toml (nÃ£o utilizados)
- âœ… Estrutura limpa e organizada
- âœ… Apenas arquivos essenciais mantidos

---

## ğŸ“ Estrutura Final

```
vmpro1/
â”œâ”€â”€ README.md              # DocumentaÃ§Ã£o completa (Ãºnica fonte de verdade)
â”œâ”€â”€ QUICKSTART.md          # Guia rÃ¡pido de comandos
â”œâ”€â”€ .env.example           # Template de variÃ¡veis
â”œâ”€â”€ utils.sh               # CLI Ãºnica com todos os comandos
â”œâ”€â”€ docker-compose.yml     # ConfiguraÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ test_capacity.py       # Testes de capacidade
â”œâ”€â”€ monitor_memory.sh      # Monitor de memÃ³ria em tempo real
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/              # 4 DAGs prontas
â”‚   â”œâ”€â”€ logs/              # Logs do Airflow
â”‚   â””â”€â”€ plugins/           # Plugins customizados
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ jobs/              # Jobs Spark (data_quality_check, sales_aggregation)
â”‚   â””â”€â”€ notebooks/         # Notebooks Jupyter
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ init/              # Scripts de inicializaÃ§Ã£o do banco
â”‚   â””â”€â”€ analytics_queries.sql
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ dashboard.py       # Dashboard Streamlit completo
â””â”€â”€ data/                  # Dados (raw/staging/processed)
```

---

## ğŸ¯ Comandos Principais

### InÃ­cio RÃ¡pido
```bash
./utils.sh start    # Inicia tudo
./utils.sh status   # Verifica status
./utils.sh health   # Health check completo
```

### Gerenciamento
```bash
./utils.sh stop     # Para tudo
./utils.sh restart  # Reinicia
./utils.sh reset    # Reset completo (limpa volumes)
```

### Monitoramento
```bash
./utils.sh memory      # Uso de RAM
./utils.sh monitor     # Monitor em tempo real
./utils.sh data-count  # Contagem de registros
./utils.sh logs        # Ver todos os logs
```

### Dados e Testes
```bash
./utils.sh test 1000 50 1  # Teste: 50K registros
./utils.sh clean-data      # Limpar dados de teste
```

---

## ğŸŒ Endpoints

| ServiÃ§o | URL | Credenciais |
|---------|-----|-------------|
| **Dashboard** | http://localhost:8501 | - |
| **Airflow** | http://localhost:8080 | admin/admin |
| **Spark UI** | http://localhost:8081 | - |
| **MinIO** | http://localhost:9001 | minioadmin/minioadmin |
| **PostgreSQL** | localhost:5432 | postgres/postgres |

---

## ğŸ“Š Status Atual

### âœ… ServiÃ§os Funcionando
- [x] PostgreSQL 15 (128 MB)
- [x] MinIO (96 MB)
- [x] Spark Master (128 MB)
- [x] Spark Worker (128 MB)
- [x] Airflow Webserver (512 MB)
- [x] Airflow Scheduler (300 MB)
- [x] Dashboard Streamlit (128 MB)

### ğŸ“ˆ MÃ©tricas
- **Total RAM:** ~1.3 GB
- **Containers:** 7 ativos
- **DAGs:** 4 disponÃ­veis
- **Capacidade testada:** 50,000 registros em <1 min
- **Taxa de ingestÃ£o:** ~870 registros/segundo

### ğŸ² Dados de Teste Atual
- **Raw Transactions:** 50,000
- **Raw Customers:** 8,966
- **Raw Products:** 900

---

## ğŸ“š DocumentaÃ§Ã£o

### README.md (Principal)
- VisÃ£o geral completa
- Arquitetura detalhada
- Guia de uso
- Arquitetura de dados (4 camadas)
- Testes de capacidade
- Troubleshooting completo

### QUICKSTART.md
- Comandos essenciais
- Fluxo de trabalho bÃ¡sico
- Atalhos Ãºteis

### .env.example
- Todas as variÃ¡veis de ambiente
- Valores padrÃ£o documentados
- Guia de configuraÃ§Ã£o

---

## ğŸ”§ Melhorias Implementadas

1. **DocumentaÃ§Ã£o Ãšnica**
   - Antes: 10+ arquivos dispersos
   - Depois: 2 arquivos principais (README + QUICKSTART)

2. **Scripts Consolidados**
   - Antes: 5+ scripts bash separados
   - Depois: 1 CLI unificado (utils.sh)

3. **Comandos Intuitivos**
   - Antes: Precisava saber comandos Docker
   - Depois: `./utils.sh <comando>` auto-explicativo

4. **Health Checks**
   - VerificaÃ§Ã£o automÃ¡tica de todos os serviÃ§os
   - DetecÃ§Ã£o de problemas rÃ¡pida

5. **Testes Automatizados**
   - Scripts de teste de capacidade
   - Monitor de memÃ³ria em tempo real

---

## ğŸš€ PrÃ³ximos Passos Sugeridos

1. **Performance**
   - [ ] Otimizar queries SQL com indexes
   - [ ] Tune de parÃ¢metros Spark
   - [ ] Cache de dados frequentes

2. **Observabilidade**
   - [ ] Integrar Prometheus + Grafana
   - [ ] Logs centralizados (ELK Stack)
   - [ ] Alertas automÃ¡ticos

3. **SeguranÃ§a**
   - [ ] Credenciais via secrets
   - [ ] HTTPS nos endpoints
   - [ ] Network policies

4. **CI/CD**
   - [ ] GitHub Actions
   - [ ] Testes automatizados
   - [ ] Deploy automatizado

5. **Features**
   - [ ] Adicionar Metabase para BI
   - [ ] Data lineage tracking
   - [ ] Data quality framework

---

## ğŸ’¡ Dicas de Uso

### Para Desenvolvimento
```bash
# Iniciar sem Dashboard (economiza 128MB)
docker-compose up -d postgres minio spark-master spark-worker airflow-webserver airflow-scheduler

# Ver logs em tempo real
./utils.sh logs airflow_webserver

# Monitorar performance
./utils.sh monitor
```

### Para Testes
```bash
# Teste rÃ¡pido (10K registros)
./utils.sh test 1000 10 1

# Teste mÃ©dio (50K registros)
./utils.sh test 2000 25 1

# Limpar depois
./utils.sh clean-data
```

### Para ProduÃ§Ã£o
```bash
# Alterar credenciais em .env
cp .env.example .env
vim .env

# Iniciar com volumes persistentes
./utils.sh start

# Monitorar saÃºde
watch -n 10 './utils.sh health'
```

---

## ğŸ“ Suporte

- **DocumentaÃ§Ã£o:** README.md e QUICKSTART.md
- **Ajuda:** `./utils.sh help`
- **Logs:** `./utils.sh logs [servico]`
- **Status:** `./utils.sh status`

---

**âœ¨ Projeto otimizado e pronto para uso!**
