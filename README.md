# Pipeline de Engenharia de Dados - High Level

## ğŸ“Š Arquitetura do Projeto

Este Ã© um projeto completo de engenharia de dados que implementa uma pipeline moderna de ETL/ELT utilizando:

- **IngestÃ£o de Dados**: Apache Kafka (lightweight) ou API REST
- **Processamento**: Apache Spark (PySpark) em modo standalone
- **Armazenamento**: PostgreSQL + MinIO (S3-compatible)
- **OrquestraÃ§Ã£o**: Apache Airflow (lightweight mode)
- **Monitoramento**: Prometheus + Grafana (opcional, pode ser desabilitado)
- **VisualizaÃ§Ã£o**: Metabase

## ğŸš€ EspecificaÃ§Ãµes da VM

- **CPU**: 2 cores AMD EPYC 7551
- **RAM**: ~1GB (otimizado para uso mÃ­nimo de memÃ³ria)
- **Arquitetura**: x86_64
- **OS**: Ubuntu

## ğŸ“ Estrutura do Projeto

```
vmpro1/
â”œâ”€â”€ docker-compose.yml          # OrquestraÃ§Ã£o dos containers
â”œâ”€â”€ airflow/                    # DAGs e configuraÃ§Ãµes Airflow
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ spark/                      # Jobs Spark
â”‚   â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ notebooks/
â”œâ”€â”€ data/                       # Dados brutos e processados
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ staging/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ sql/                        # Scripts SQL
â”‚   â””â”€â”€ init/
â”œâ”€â”€ config/                     # ConfiguraÃ§Ãµes
â””â”€â”€ monitoring/                 # Dashboards e configs
```

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.11**: Linguagem principal
- **Apache Airflow 2.8**: OrquestraÃ§Ã£o de workflows
- **Apache Spark 3.5**: Processamento distribuÃ­do
- **PostgreSQL 15**: Data warehouse
- **MinIO**: Object storage (S3-compatible)
- **Redis**: Message broker para Airflow
- **Metabase**: BI e visualizaÃ§Ã£o

## ğŸ¯ Casos de Uso

1. **ETL de E-commerce**: IngestÃ£o e processamento de dados de vendas
2. **AnÃ¡lise de Logs**: Processamento e anÃ¡lise de logs de aplicaÃ§Ãµes
3. **Data Quality**: ValidaÃ§Ã£o e qualidade de dados
4. **AgregaÃ§Ãµes**: CriaÃ§Ã£o de mÃ©tricas e KPIs

## ğŸ“¦ Como Executar

```bash
# 1. Construir e iniciar os containers
docker-compose up -d

# 2. Acessar interfaces:
# - Airflow: http://localhost:8080 (admin/admin)
# - Metabase: http://localhost:3000
# - MinIO: http://localhost:9001 (minioadmin/minioadmin)

# 3. Executar pipeline de exemplo
docker-compose exec airflow airflow dags trigger example_etl_pipeline
```

## ğŸ”§ OtimizaÃ§Ãµes para Low Memory

- Airflow em modo Sequential Executor (sem paralelizaÃ§Ã£o)
- Spark com memÃ³ria limitada (512MB executor)
- PostgreSQL com shared_buffers reduzidos
- Desabilitar componentes opcionais quando necessÃ¡rio

## ğŸ“Š Pipeline de Dados

1. **IngestÃ£o**: Dados brutos sÃ£o coletados de APIs/arquivos
2. **Staging**: Dados sÃ£o armazenados temporariamente no MinIO
3. **TransformaÃ§Ã£o**: Spark processa e transforma os dados
4. **Load**: Dados processados vÃ£o para PostgreSQL
5. **VisualizaÃ§Ã£o**: Metabase consome dados do PostgreSQL

## ğŸ” Credenciais PadrÃ£o

- **Airflow**: admin / admin
- **PostgreSQL**: postgres / postgres
- **MinIO**: minioadmin / minioadmin
- **Metabase**: Configure no primeiro acesso
# vmproeng
