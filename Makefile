# Makefile para Data Engineering Project

.PHONY: help start stop restart logs clean install test backup restore

# VariÃ¡veis
DOCKER_COMPOSE = docker-compose
AIRFLOW_CONTAINER = airflow-webserver
POSTGRES_CONTAINER = postgres

help: ## Mostra esta mensagem de ajuda
	@echo "Data Engineering Project - Comandos DisponÃ­veis:"
	@echo ""
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-20s\033[0m %s\n", $$1, $$2}'

start: ## Inicia todos os serviÃ§os
	@echo "ðŸš€ Iniciando serviÃ§os..."
	@chmod +x start.sh
	@./start.sh

stop: ## Para todos os serviÃ§os
	@echo "ðŸ›‘ Parando serviÃ§os..."
	@$(DOCKER_COMPOSE) down

stop-clean: ## Para todos os serviÃ§os e remove volumes
	@echo "ðŸ§¹ Parando serviÃ§os e limpando dados..."
	@chmod +x stop.sh
	@./stop.sh --clean

restart: ## Reinicia todos os serviÃ§os
	@echo "ðŸ”„ Reiniciando serviÃ§os..."
	@$(DOCKER_COMPOSE) restart

logs: ## Mostra logs de todos os serviÃ§os
	@$(DOCKER_COMPOSE) logs -f

logs-airflow: ## Mostra logs do Airflow
	@$(DOCKER_COMPOSE) logs -f airflow-webserver airflow-scheduler

logs-spark: ## Mostra logs do Spark
	@$(DOCKER_COMPOSE) logs -f spark-master spark-worker

logs-postgres: ## Mostra logs do PostgreSQL
	@$(DOCKER_COMPOSE) logs -f postgres

status: ## Mostra status dos containers
	@$(DOCKER_COMPOSE) ps

stats: ## Mostra uso de recursos
	@docker stats --no-stream

shell-airflow: ## Abre shell no container Airflow
	@$(DOCKER_COMPOSE) exec $(AIRFLOW_CONTAINER) bash

shell-postgres: ## Abre shell no PostgreSQL
	@$(DOCKER_COMPOSE) exec $(POSTGRES_CONTAINER) psql -U postgres -d datawarehouse

shell-spark: ## Abre shell no Spark Master
	@$(DOCKER_COMPOSE) exec spark-master bash

trigger-etl: ## Executa pipeline ETL principal
	@echo "â–¶ï¸  Executando pipeline ETL..."
	@$(DOCKER_COMPOSE) exec $(AIRFLOW_CONTAINER) airflow dags trigger ecommerce_etl_pipeline

trigger-ingestion: ## Executa pipeline de ingestÃ£o de API
	@echo "â–¶ï¸  Executando ingestÃ£o de dados..."
	@$(DOCKER_COMPOSE) exec $(AIRFLOW_CONTAINER) airflow dags trigger api_ingestion

trigger-quality: ## Executa verificaÃ§Ã£o de qualidade
	@echo "â–¶ï¸  Executando verificaÃ§Ã£o de qualidade..."
	@$(DOCKER_COMPOSE) exec $(AIRFLOW_CONTAINER) airflow dags trigger data_quality_check

list-dags: ## Lista todas as DAGs
	@$(DOCKER_COMPOSE) exec $(AIRFLOW_CONTAINER) airflow dags list

backup-db: ## Faz backup do banco de dados
	@echo "ðŸ’¾ Criando backup do banco de dados..."
	@mkdir -p backups
	@$(DOCKER_COMPOSE) exec -T $(POSTGRES_CONTAINER) pg_dump -U postgres datawarehouse > backups/datawarehouse_$$(date +%Y%m%d_%H%M%S).sql
	@echo "âœ… Backup criado em backups/"

restore-db: ## Restaura backup do banco de dados (uso: make restore-db FILE=backup.sql)
	@echo "ðŸ“¥ Restaurando backup do banco de dados..."
	@$(DOCKER_COMPOSE) exec -T $(POSTGRES_CONTAINER) psql -U postgres datawarehouse < $(FILE)
	@echo "âœ… Backup restaurado"

clean-data: ## Limpa dados temporÃ¡rios
	@echo "ðŸ§¹ Limpando dados temporÃ¡rios..."
	@rm -rf data/raw/*
	@rm -rf data/staging/*
	@rm -rf data/processed/*
	@rm -rf airflow/logs/*
	@echo "âœ… Dados temporÃ¡rios limpos"

install: ## Instala dependÃªncias Python localmente
	@echo "ðŸ“¦ Instalando dependÃªncias..."
	@pip install -r requirements.txt

test-connection: ## Testa conexÃ£o com banco de dados
	@echo "ðŸ” Testando conexÃ£o com PostgreSQL..."
	@$(DOCKER_COMPOSE) exec $(POSTGRES_CONTAINER) psql -U postgres -d datawarehouse -c "SELECT version();"

create-user: ## Cria usuÃ¡rio admin no Airflow
	@echo "ðŸ‘¤ Criando usuÃ¡rio admin..."
	@$(DOCKER_COMPOSE) exec $(AIRFLOW_CONTAINER) airflow users create \
		--username admin \
		--password admin \
		--firstname Admin \
		--lastname User \
		--role Admin \
		--email admin@example.com

query: ## Executa query SQL (uso: make query SQL="SELECT * FROM table")
	@$(DOCKER_COMPOSE) exec $(POSTGRES_CONTAINER) psql -U postgres -d datawarehouse -c "$(SQL)"

monitor: ## Abre dashboard de monitoramento
	@echo "ðŸ“Š Abrindo URLs de monitoramento..."
	@echo "Airflow:    http://localhost:8080"
	@echo "Spark:      http://localhost:8081"
	@echo "MinIO:      http://localhost:9001"
	@echo "Metabase:   http://localhost:3000"

build: ## ConstrÃ³i as imagens Docker
	@$(DOCKER_COMPOSE) build

pull: ## Baixa as imagens Docker
	@$(DOCKER_COMPOSE) pull

update: ## Atualiza o projeto (pull + restart)
	@echo "ðŸ”„ Atualizando projeto..."
	@git pull
	@$(DOCKER_COMPOSE) pull
	@$(DOCKER_COMPOSE) up -d
	@echo "âœ… Projeto atualizado"

validate-dags: ## Valida sintaxe das DAGs
	@echo "âœ… Validando DAGs..."
	@$(DOCKER_COMPOSE) exec $(AIRFLOW_CONTAINER) python -m py_compile /opt/airflow/dags/*.py
	@echo "âœ… DAGs validadas com sucesso"
